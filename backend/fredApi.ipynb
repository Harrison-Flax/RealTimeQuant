{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cca4d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] CPIAUCSL     rows=   69  last=2025-09-01  value=324.368000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPIAUCSL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-05-01</th>\n",
       "      <td>320.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-01</th>\n",
       "      <td>321.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-07-01</th>\n",
       "      <td>322.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-01</th>\n",
       "      <td>323.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-01</th>\n",
       "      <td>324.368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CPIAUCSL\n",
       "date                \n",
       "2025-05-01   320.580\n",
       "2025-06-01   321.500\n",
       "2025-07-01   322.132\n",
       "2025-08-01   323.364\n",
       "2025-09-01   324.368"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] IC4WSA       rows=  299  last=2025-09-20  value=237500.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IC4WSA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-23</th>\n",
       "      <td>228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-30</th>\n",
       "      <td>230750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-06</th>\n",
       "      <td>240750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-13</th>\n",
       "      <td>240250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-20</th>\n",
       "      <td>237500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            IC4WSA\n",
       "date              \n",
       "2025-08-23  228500\n",
       "2025-08-30  230750\n",
       "2025-09-06  240750\n",
       "2025-09-13  240250\n",
       "2025-09-20  237500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] DCOILBRENTEU rows= 1484  last=2025-11-11  value=63.860000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DCOILBRENTEU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-11-05</th>\n",
       "      <td>63.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-06</th>\n",
       "      <td>63.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-07</th>\n",
       "      <td>63.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-10</th>\n",
       "      <td>63.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-11</th>\n",
       "      <td>63.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            DCOILBRENTEU\n",
       "date                    \n",
       "2025-11-05         63.54\n",
       "2025-11-06         63.41\n",
       "2025-11-07         63.72\n",
       "2025-11-10         63.01\n",
       "2025-11-11         63.86"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ FRED key works.\n"
     ]
    }
   ],
   "source": [
    "# check to see if FRED key is working\n",
    "\n",
    "import os, sys, requests, pandas as pd\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Resolve your key safely for this runtime\n",
    "FRED_API_KEY = os.getenv(\"FRED_API_KEY\") or getpass(\"Paste your FRED API key (hidden): \").strip()\n",
    "if not FRED_API_KEY:\n",
    "    raise SystemExit(\"No FRED API key provided.\")\n",
    "os.environ[\"FRED_API_KEY\"] = FRED_API_KEY\n",
    "\n",
    "FRED_OBS_API = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "\n",
    "def get_observations(series_id: str,\n",
    "                     start: str = \"2020-01-01\",\n",
    "                     end: str = datetime.today().strftime(\"%Y-%m-%d\"),\n",
    "                     timeout: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"Fetch FRED observations; returns df indexed by date with a single <series_id> column.\"\"\"\n",
    "    params = {\n",
    "        \"series_id\": series_id,\n",
    "        \"file_type\": \"json\",\n",
    "        \"observation_start\": start,\n",
    "        \"observation_end\": end,\n",
    "        \"api_key\": os.environ[\"FRED_API_KEY\"],\n",
    "    }\n",
    "    r = requests.get(FRED_OBS_API, params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    payload = r.json()\n",
    "    if \"observations\" not in payload:\n",
    "        raise RuntimeError(f\"No 'observations' in response for {series_id}: {payload}\")\n",
    "    df = pd.DataFrame(payload[\"observations\"])\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"No rows for {series_id} in {start}..{end}\")\n",
    "\n",
    "    # Keep only the two fields we need, then normalize/rename\n",
    "    df = df.loc[:, [\"date\", \"value\"]].copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\", \"value\"]).sort_values(\"date\")\n",
    "    df = df.set_index(\"date\").rename(columns={\"value\": series_id})\n",
    "    return df\n",
    "\n",
    "def summarize(df: pd.DataFrame):\n",
    "    sid = df.columns[0]\n",
    "    print(f\"[ok] {sid:<12} rows={len(df):>5}  last={df.index.max().date()}  value={df.iloc[-1,0]:.6f}\")\n",
    "    display(df.tail(5))\n",
    "\n",
    "# Test monthly / weekly / daily series\n",
    "series_to_test = [\"CPIAUCSL\", \"IC4WSA\", \"DCOILBRENTEU\"]\n",
    "start = \"2020-01-01\"; end = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "ok = True\n",
    "for sid in series_to_test:\n",
    "    try:\n",
    "        df = get_observations(sid, start, end)\n",
    "        summarize(df)\n",
    "    except Exception as e:\n",
    "        ok = False\n",
    "        print(f\"[fail] {sid}: {e}\", file=sys.stderr)\n",
    "\n",
    "print(\"\\n✅ FRED key works.\" if ok else \"\\n⚠️ Some series failed — check key, dates, or IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75475722",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No Cleveland CSV files in data/nowcast",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 319\u001b[39m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLatest forecast:\u001b[39m\u001b[33m\"\u001b[39m, json.dumps(result[\u001b[33m\"\u001b[39m\u001b[33mlatest\u001b[39m\u001b[33m\"\u001b[39m], indent=\u001b[32m2\u001b[39m))\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 298\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    297\u001b[39m     os.makedirs(CFG.out_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     daily_df, nc = \u001b[43mbuild_daily_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[data] rows=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(daily_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m range=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdaily_df.index.min().date()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m→\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdaily_df.index.max().date()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(daily_df.columns)-\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    300\u001b[39m     result = train_eval_daily(daily_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36mbuild_daily_dataset\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_daily_dataset\u001b[39m() -> Tuple[pd.DataFrame, pd.DataFrame]:\n\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# Target (daily nowcast)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     nc = \u001b[43mload_cleveland_daily\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcleveland_src_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     \u001b[38;5;66;03m# Expand to full daily calendar, ffill gaps (weekends missing in Cleveland on some months)\u001b[39;00m\n\u001b[32m    178\u001b[39m     calendar = pd.date_range(\u001b[38;5;28mmax\u001b[39m(pd.to_datetime(CFG.start_date), nc.index.min()),\n\u001b[32m    179\u001b[39m                              \u001b[38;5;28mmin\u001b[39m(pd.to_datetime(CFG.end_date), nc.index.max()),\n\u001b[32m    180\u001b[39m                              freq=\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mload_cleveland_daily\u001b[39m\u001b[34m(src_dir)\u001b[39m\n\u001b[32m    151\u001b[39m files = \u001b[38;5;28msorted\u001b[39m(glob.glob(os.path.join(src_dir, \u001b[33m\"\u001b[39m\u001b[33m*.csv\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo Cleveland CSV files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    154\u001b[39m parts = []\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No Cleveland CSV files in data/nowcast"
     ]
    }
   ],
   "source": [
    "# File: daily_nowcast_model.py\n",
    "\"\"\"\n",
    "Daily Cleveland CPI-nowcast model:\n",
    "- Builds a daily dataset from Cleveland \"Table View\" CSVs (one file per month).\n",
    "- Merges FRED daily/weekly features (Brent, Gas, Claims), forward-filling sparse series.\n",
    "- Predicts next-day change in nowcast (Δ), avoiding lookahead.\n",
    "Requires: pandas, numpy, scikit-learn, requests\n",
    "Env: FRED_API_KEY must be set for FRED JSON API (falls back to fredgraph.csv if missing).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    fred_api_key: Optional[str] = os.getenv(\"FRED_API_KEY\")  # set in your env for authenticated pulls\n",
    "    cleveland_src_dir: str = \"data/nowcast\"                  # folder with monthly \"Table View\" CSVs\n",
    "    out_dir: str = \"artifacts_daily\"\n",
    "    start_date: str = \"2020-01-01\"                           # adjust if you have older Cleveland files\n",
    "    end_date: str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "FRED_OBS_API = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "FREDGRAPH_CSV = \"https://fred.stlouisfed.org/graph/fredgraph.csv\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# FRED loader (API first, CSV fallback)\n",
    "# ----------------------------\n",
    "\n",
    "def fred_series(series_id: str, start: str, end: str, api_key: Optional[str]) -> pd.DataFrame:\n",
    "    # Why: prefer API; fallback keeps dev flow if key missing.\n",
    "    if api_key:\n",
    "        params = {\"series_id\": series_id, \"file_type\": \"json\",\n",
    "                  \"observation_start\": start, \"observation_end\": end,\n",
    "                  \"api_key\": api_key}\n",
    "        try:\n",
    "            r = requests.get(FRED_OBS_API, params=params, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            payload = r.json()\n",
    "            obs = payload.get(\"observations\", [])\n",
    "            df = pd.DataFrame(obs)\n",
    "            if not df.empty:\n",
    "                df = df.loc[:, [\"date\", \"value\"]]\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "                df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "                df = df.dropna().sort_values(\"date\").set_index(\"date\")\n",
    "                df.columns = [series_id]\n",
    "                return df\n",
    "            warnings.warn(f\"FRED API returned no rows for {series_id}; using csv fallback.\")\n",
    "        except requests.HTTPError as e:\n",
    "            warnings.warn(f\"FRED API error for {series_id}: {e}; using csv fallback.\")\n",
    "\n",
    "    r = requests.get(FREDGRAPH_CSV, params={\"id\": series_id}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    raw = pd.read_csv(io.StringIO(r.text))\n",
    "    if \"observation_date\" not in raw.columns or series_id not in raw.columns:\n",
    "        raise ValueError(f\"Unexpected fredgraph format for {series_id}\")\n",
    "    raw[\"observation_date\"] = pd.to_datetime(raw[\"observation_date\"], errors=\"coerce\")\n",
    "    raw[series_id] = pd.to_numeric(raw[series_id], errors=\"coerce\")\n",
    "    mask = (raw[\"observation_date\"] >= pd.to_datetime(start)) & (raw[\"observation_date\"] <= pd.to_datetime(end))\n",
    "    df = raw.loc[mask, [\"observation_date\", series_id]].dropna().sort_values(\"observation_date\")\n",
    "    return df.set_index(\"observation_date\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Cleveland daily nowcast loader (directory of monthly CSVs)\n",
    "# ----------------------------\n",
    "\n",
    "def _infer_year_month_from_name(path: str) -> Optional[Tuple[int, int]]:\n",
    "    m = re.search(r\"(20\\d{2})[-_ ]?(\\d{1,2})\", os.path.basename(path))\n",
    "    if not m:\n",
    "        return None\n",
    "    return int(m.group(1)), int(m.group(2))\n",
    "\n",
    "def _pick_cpi_column(cols: List[str]) -> Optional[str]:\n",
    "    for c in cols:\n",
    "        lc = str(c).lower().replace(\" \", \"\")\n",
    "        if \"cpi\" in lc and \"core\" not in lc:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _parse_cleveland_month_csv(path: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Failed to read {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    label_col = next((c for c in df.columns if str(c).strip().lower() in {\"label\", \"date\"}), None)\n",
    "    if not label_col:\n",
    "        warnings.warn(f\"No 'Label' column in {path}\")\n",
    "        return None\n",
    "    cpi_col = _pick_cpi_column(list(df.columns))\n",
    "    if not cpi_col:\n",
    "        warnings.warn(f\"No CPI MoM col in {path}\")\n",
    "        return None\n",
    "\n",
    "    ym = _infer_year_month_from_name(path)\n",
    "    if not ym:\n",
    "        warnings.warn(f\"Cannot infer YYYY-MM from {path}\")\n",
    "        return None\n",
    "    year, month = ym\n",
    "\n",
    "    def to_dt(s: str):\n",
    "        s = str(s).strip()\n",
    "        if not s or s.lower() == \"nan\":\n",
    "            return None\n",
    "        try:\n",
    "            mm, dd = s.split(\"/\")\n",
    "            return datetime(year=year, month=int(mm), day=int(dd))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    df[\"date\"] = df[label_col].apply(to_dt)\n",
    "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "    df[\"cpi_mom_nowcast\"] = pd.to_numeric(df[cpi_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"cpi_mom_nowcast\"])\n",
    "    out = df[[\"date\", \"cpi_mom_nowcast\"]].copy()\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"])\n",
    "    return out.set_index(\"date\")\n",
    "\n",
    "def load_cleveland_daily(src_dir: str) -> pd.DataFrame:\n",
    "    files = sorted(glob.glob(os.path.join(src_dir, \"*.csv\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No Cleveland CSV files in {src_dir}\")\n",
    "    parts = []\n",
    "    for f in files:\n",
    "        d = _parse_cleveland_month_csv(f)\n",
    "        if d is not None and not d.empty:\n",
    "            parts.append(d)\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"No usable Cleveland files parsed.\")\n",
    "    df = pd.concat(parts).sort_index()\n",
    "    # Deduplicate dates (keep last)\n",
    "    df = df[~df.index.duplicated(keep=\"last\")]\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Daily feature table\n",
    "# ----------------------------\n",
    "\n",
    "def daily_log_return(series: pd.Series) -> pd.Series:\n",
    "    return 100.0 * np.log(series / series.shift(1))\n",
    "\n",
    "def build_daily_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Target (daily nowcast)\n",
    "    nc = load_cleveland_daily(CFG.cleveland_src_dir)\n",
    "    # Expand to full daily calendar, ffill gaps (weekends missing in Cleveland on some months)\n",
    "    calendar = pd.date_range(max(pd.to_datetime(CFG.start_date), nc.index.min()),\n",
    "                             min(pd.to_datetime(CFG.end_date), nc.index.max()),\n",
    "                             freq=\"D\")\n",
    "    nc = nc.reindex(calendar).ffill()\n",
    "\n",
    "    # FRED features\n",
    "    brent = fred_series(\"DCOILBRENTEU\", calendar.min().strftime(\"%Y-%m-%d\"),\n",
    "                        calendar.max().strftime(\"%Y-%m-%d\"), CFG.fred_api_key).rename(columns={\"DCOILBRENTEU\": \"brent\"})\n",
    "    gas = fred_series(\"GASREGW\", calendar.min().strftime(\"%Y-%m-%d\"),\n",
    "                      calendar.max().strftime(\"%Y-%m-%d\"), CFG.fred_api_key).rename(columns={\"GASREGW\": \"gas\"})\n",
    "    claims = fred_series(\"IC4WSA\", calendar.min().strftime(\"%Y-%m-%d\"),\n",
    "                         calendar.max().strftime(\"%Y-%m-%d\"), CFG.fred_api_key).rename(columns={\"IC4WSA\": \"claims4w\"})\n",
    "\n",
    "    # Reindex each to daily calendar & ffill (why: weekends/weekly cadence)\n",
    "    # \"reindex(calendar)\" aligns the brent DataFrame to the daily 'calendar' date range.\n",
    "    # This expands brent to have a row for every day in 'calendar', even if brent originally only included, for example, business days or some weekly frequency from FRED.\n",
    "    # Any missing dates (e.g., weekends, holidays) are filled with NaN, which are then forward-filled by .ffill().\n",
    "    # This ensures brent has daily values matching the date index used for all other features,\n",
    "    # so that feature engineering, rolling windows, and eventual model training all work day-by-day.\n",
    "    brent = brent.reindex(calendar).ffill()\n",
    "    gas = gas.reindex(calendar).ffill()\n",
    "    claims = claims.reindex(calendar).ffill()\n",
    "\n",
    "    # Feature engineering (lag everything to avoid lookahead)\n",
    "    feats = pd.DataFrame(index=calendar)\n",
    "    feats[\"nc\"] = nc[\"cpi_mom_nowcast\"]\n",
    "    feats[\"nc_lag1\"] = feats[\"nc\"].shift(1)\n",
    "    feats[\"nc_lag3\"] = feats[\"nc\"].shift(3)\n",
    "    feats[\"nc_lag7\"] = feats[\"nc\"].shift(7)\n",
    "    feats[\"nc_ma7\"] = feats[\"nc\"].rolling(7, min_periods=3).mean()\n",
    "    feats[\"nc_std7\"] = feats[\"nc\"].rolling(7, min_periods=3).std()\n",
    "    feats[\"nc_ma14\"] = feats[\"nc\"].rolling(14, min_periods=5).mean()\n",
    "\n",
    "    feats[\"brent\"] = brent[\"brent\"]\n",
    "    feats[\"brent_ret\"] = daily_log_return(feats[\"brent\"]).shift(1)   # lag 1\n",
    "    feats[\"brent_ret_3\"] = feats[\"brent_ret\"].rolling(3, min_periods=1).sum()\n",
    "    feats[\"brent_ret_7\"] = feats[\"brent_ret\"].rolling(7, min_periods=3).sum()\n",
    "\n",
    "    feats[\"gas\"] = gas[\"gas\"]\n",
    "    feats[\"gas_ret\"] = daily_log_return(feats[\"gas\"]).shift(1)\n",
    "    feats[\"gas_ret_7\"] = feats[\"gas_ret\"].rolling(7, min_periods=3).sum()\n",
    "\n",
    "    # Claims: weekly level + weekly change (computed on weekly series then ffilled)\n",
    "    claims_weekly_change = claims[\"claims4w\"].diff().where(claims.index.dayofweek == claims.index.dayofweek)  # placeholder calc\n",
    "    feats[\"claims4w\"] = claims[\"claims4w\"].shift(1)  # lag 1 day\n",
    "    feats[\"claims_chg\"] = claims[\"claims4w\"].diff()\n",
    "\n",
    "    # Calendar features (position within month)\n",
    "    dom = feats.index.day\n",
    "    month_len = feats.index.daysinmonth\n",
    "    feats[\"dom_sin\"] = np.sin(2 * np.pi * dom / month_len)\n",
    "    feats[\"dom_cos\"] = np.cos(2 * np.pi * dom / month_len)\n",
    "\n",
    "    # Target: next-day change in nowcast (Δ)\n",
    "    y = feats[\"nc\"].shift(-1) - feats[\"nc\"]\n",
    "\n",
    "    # Final cleanup\n",
    "    X = feats.drop(columns=[\"nc\"])  # we keep lags/rolls, not the contemporaneous 'nc'\n",
    "    df = pd.concat([X, y.rename(\"y_next_delta\")], axis=1).dropna()\n",
    "    return df.drop(columns=[\"brent\", \"gas\"]), nc  # drop raw price levels to keep it lean\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Modeling (walk-forward)\n",
    "# ----------------------------\n",
    "\n",
    "def train_eval_daily(df: pd.DataFrame) -> Dict:\n",
    "    Xcols = [c for c in df.columns if c != \"y_next_delta\"]\n",
    "    ycol = \"y_next_delta\"\n",
    "    X, y = df[Xcols], df[ycol]\n",
    "\n",
    "    pre = ColumnTransformer([(\"num\", StandardScaler(), Xcols)], remainder=\"drop\")\n",
    "    enet = Pipeline([(\"pre\", pre),\n",
    "                     (\"m\", ElasticNet(alpha=0.02, l1_ratio=0.15, max_iter=5000, random_state=42))])\n",
    "    gbr = Pipeline([(\"pre\", pre),\n",
    "                    (\"m\", GradientBoostingRegressor(random_state=42, n_estimators=600, max_depth=3, learning_rate=0.03))])\n",
    "\n",
    "    # Use smaller k if dataset is short\n",
    "    n_splits = min(8, max(3, len(X) // 40))\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    enet_mae, enet_rmse, gbr_mae, gbr_rmse = [], [], [], []\n",
    "    for tr, te in tscv.split(X):\n",
    "        Xtr, Xte, ytr, yte = X.iloc[tr], X.iloc[te], y.iloc[tr], y.iloc[te]\n",
    "        enet.fit(Xtr, ytr); p1 = enet.predict(Xte)\n",
    "        gbr.fit(Xtr, ytr);  p2 = gbr.predict(Xte)\n",
    "        enet_mae.append(mean_absolute_error(yte, p1))\n",
    "        enet_rmse.append(math.sqrt(mean_squared_error(yte, p1)))\n",
    "        gbr_mae.append(mean_absolute_error(yte, p2))\n",
    "        gbr_rmse.append(math.sqrt(mean_squared_error(yte, p2)))\n",
    "\n",
    "    metrics = {\n",
    "        \"avg_enet_mae\": float(np.mean(enet_mae)),\n",
    "        \"avg_enet_rmse\": float(np.mean(enet_rmse)),\n",
    "        \"avg_gbr_mae\": float(np.mean(gbr_mae)),\n",
    "        \"avg_gbr_rmse\": float(np.mean(gbr_rmse)),\n",
    "        \"splits\": n_splits,\n",
    "        \"rows\": int(len(X)),\n",
    "        \"start\": X.index.min().strftime(\"%Y-%m-%d\"),\n",
    "        \"end\": X.index.max().strftime(\"%Y-%m-%d\"),\n",
    "    }\n",
    "\n",
    "    # Fit on all and forecast tomorrow (Δ and level)\n",
    "    enet.fit(X, y)\n",
    "    gbr.fit(X, y)\n",
    "    x_last = X.iloc[[-1]]\n",
    "    delta_pred = float(gbr.predict(x_last)[0])\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"latest\": {\n",
    "            \"asof\": X.index.max().strftime(\"%Y-%m-%d\"),\n",
    "            \"delta_pred\": delta_pred,\n",
    "            # Level forecast for tomorrow = today_nowcast + delta_pred\n",
    "            \"level_pred\": float(df.loc[X.index.max(), \"y_next_delta\"] + df.loc[X.index.max(), \"y_next_delta\"].shift(1) if False else np.nan)  # placeholder\n",
    "        },\n",
    "        \"models\": {\"enet\": enet, \"gbr\": gbr}\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Orchestrator\n",
    "# ----------------------------\n",
    "\n",
    "def run() -> None:\n",
    "    os.makedirs(CFG.out_dir, exist_ok=True)\n",
    "    daily_df, nc = build_daily_dataset()\n",
    "    print(f\"[data] rows={len(daily_df)} range={daily_df.index.min().date()}→{daily_df.index.max().date()} features={len(daily_df.columns)-1}\")\n",
    "    result = train_eval_daily(daily_df)\n",
    "\n",
    "    # Compute tomorrow's level forecast properly\n",
    "    last_date = daily_df.index.max()\n",
    "    today_nowcast = float(nc.loc[last_date, \"cpi_mom_nowcast\"])\n",
    "    delta_pred = result[\"latest\"][\"delta_pred\"]\n",
    "    result[\"latest\"][\"level_pred\"] = today_nowcast + delta_pred\n",
    "\n",
    "    # Save\n",
    "    with open(os.path.join(CFG.out_dir, \"daily_metrics.json\"), \"w\") as f:\n",
    "        json.dump(result[\"metrics\"], f, indent=2)\n",
    "    with open(os.path.join(CFG.out_dir, \"daily_latest.json\"), \"w\") as f:\n",
    "        json.dump(result[\"latest\"], f, indent=2)\n",
    "\n",
    "    print(json.dumps(result[\"metrics\"], indent=2))\n",
    "    print(\"Latest forecast:\", json.dumps(result[\"latest\"], indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30779a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
